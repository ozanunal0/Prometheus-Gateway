{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Prometheus Gateway","text":"<p>A high-performance, multi-provider LLM gateway with advanced caching, monitoring, and security features.</p>"},{"location":"#features","title":"\ud83d\ude80 Features","text":""},{"location":"#multi-provider-support","title":"Multi-Provider Support","text":"<ul> <li>OpenAI GPT - GPT-4o, GPT-3.5-turbo, and more</li> <li>Google Gemini - Gemini 2.5 Flash, Gemini 2.5 Pro</li> <li>Anthropic Claude - Claude Sonnet, Claude Opus</li> <li>Extensible Architecture - Easy to add new providers</li> </ul>"},{"location":"#intelligent-routing","title":"Intelligent Routing","text":"<ul> <li>Configuration-Driven - YAML-based provider configuration</li> <li>Model-to-Provider Mapping - Automatic routing based on model names</li> <li>Failover Support - Automatic fallback to alternative providers</li> </ul>"},{"location":"#two-level-caching-system","title":"Two-Level Caching System","text":"<ul> <li>Exact Cache - Redis-based caching for identical requests</li> <li>Semantic Cache - ChromaDB vector search for similar queries</li> <li>Configurable TTL - Customizable cache expiration times</li> <li>Cache Analytics - Monitor cache hit rates and performance</li> </ul>"},{"location":"#security-privacy","title":"Security &amp; Privacy","text":"<ul> <li>Data Loss Prevention (DLP) - Automatic PII detection and anonymization</li> <li>API Key Management - Secure key generation and validation</li> <li>Rate Limiting - Per-key request throttling</li> <li>Input Sanitization - Comprehensive request validation</li> </ul>"},{"location":"#monitoring-observability","title":"Monitoring &amp; Observability","text":"<ul> <li>Prometheus Metrics - Custom metrics for all operations</li> <li>Grafana Dashboards - Pre-built monitoring dashboards</li> <li>Structured Logging - JSON-formatted logs with correlation IDs</li> <li>Health Checks - Comprehensive service health endpoints</li> </ul>"},{"location":"#architecture","title":"\ud83c\udfd7\ufe0f Architecture","text":"<pre><code>graph TB\n    Client[Client Application] --&gt; Gateway[Prometheus Gateway]\n    Gateway --&gt; DLP[DLP Scanner]\n    Gateway --&gt; Cache[Redis Cache]\n    Gateway --&gt; Semantic[Semantic Cache]\n    Gateway --&gt; OpenAI[OpenAI Provider]\n    Gateway --&gt; Google[Google Provider]\n    Gateway --&gt; Anthropic[Anthropic Provider]\n    Gateway --&gt; Metrics[Prometheus Metrics]\n    Metrics --&gt; Grafana[Grafana Dashboard]\n    Cache --&gt; Redis[(Redis)]\n    Semantic --&gt; ChromaDB[(ChromaDB)]</code></pre>"},{"location":"#performance","title":"\ud83d\udcca Performance","text":"<ul> <li>Sub-50ms Latency - For cached responses</li> <li>99.9% Uptime - Highly available with proper deployment</li> <li>Horizontal Scaling - Stateless design for easy scaling</li> <li>Memory Efficient - Optimized for high-throughput scenarios</li> </ul>"},{"location":"#security","title":"\ud83d\udd12 Security","text":"<ul> <li>Zero-Trust Architecture - All requests require valid API keys</li> <li>PII Protection - Automatic detection and anonymization</li> <li>Audit Logging - Complete request/response audit trail</li> <li>Rate Limiting - Protection against abuse and DoS attacks</li> </ul>"},{"location":"#monitoring","title":"\ud83d\udcc8 Monitoring","text":"<p>Real-time monitoring with: - Request latency histograms - Token usage tracking - Error rate monitoring - Cache performance metrics - Provider health status</p>"},{"location":"#quick-start","title":"\ud83d\ude80 Quick Start","text":"<pre><code># Clone the repository\ngit clone https://github.com/ozanunal0/prometheus-gateway.git\ncd prometheus-gateway\n\n# Start with Docker Compose\ndocker-compose up -d\n\n# Or run locally\npip install -r requirements.txt\npython -m spacy download en_core_web_lg\nuvicorn app.main:app --reload\n</code></pre>"},{"location":"#documentation","title":"\ud83d\udcda Documentation","text":"<ul> <li>Quick Start Guide</li> <li>Configuration Reference</li> <li>API Documentation</li> <li>Testing Guide</li> </ul>"},{"location":"#contributing","title":"\ud83e\udd1d Contributing","text":"<p>We welcome contributions! Please see our Testing Guide for development information.</p>"},{"location":"#license","title":"\ud83d\udcc4 License","text":"<p>This project is licensed under the MIT License - see the LICENSE file for details.</p>"},{"location":"#support","title":"\ud83c\udd98 Support","text":"<ul> <li>\ud83d\udcd6 Documentation</li> <li>\ud83d\udc1b Issue Tracker</li> <li>\ud83d\udcac Discussions</li> </ul>"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to Prometheus Gateway will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"changelog/#unreleased","title":"[Unreleased]","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li>Comprehensive test suite with 90%+ coverage</li> <li>Professional documentation site with MkDocs</li> <li>GitHub Actions CI/CD pipeline</li> <li>Performance monitoring with custom metrics</li> <li>Security scanning and vulnerability checks</li> </ul>"},{"location":"changelog/#changed","title":"Changed","text":"<ul> <li>Improved error handling across all components</li> <li>Enhanced logging with structured format</li> <li>Updated documentation with detailed examples</li> </ul>"},{"location":"changelog/#security","title":"Security","text":"<ul> <li>Added automated security scanning with bandit</li> <li>Implemented comprehensive input validation</li> <li>Enhanced API key security measures</li> </ul>"},{"location":"changelog/#100-2024-01-06","title":"[1.0.0] - 2024-01-06","text":""},{"location":"changelog/#added_1","title":"Added","text":"<ul> <li>Multi-Provider Support</li> <li>OpenAI GPT integration (GPT-4, GPT-3.5-turbo)</li> <li>Google Gemini integration (Gemini 2.5 Flash, Pro)</li> <li>Anthropic Claude integration (Claude Sonnet, Opus)</li> <li> <p>Extensible provider architecture</p> </li> <li> <p>Intelligent Routing Engine</p> </li> <li>Configuration-driven model-to-provider mapping</li> <li>Automatic provider selection based on model names</li> <li> <p>YAML-based configuration with validation</p> </li> <li> <p>Two-Level Caching System</p> </li> <li>Redis exact cache for identical requests</li> <li>ChromaDB semantic cache for similar queries</li> <li>Configurable TTL and similarity thresholds</li> <li> <p>Cache hit rate monitoring</p> </li> <li> <p>Data Loss Prevention (DLP)</p> </li> <li>Automatic PII detection using Microsoft Presidio</li> <li>Support for emails, phone numbers, SSNs, credit cards</li> <li>Configurable anonymization strategies</li> <li> <p>Real-time scanning of all requests</p> </li> <li> <p>Security &amp; Authentication</p> </li> <li>API key management system</li> <li>Secure key generation and hashing</li> <li>Rate limiting per API key (10 requests/minute)</li> <li> <p>Request validation and sanitization</p> </li> <li> <p>Monitoring &amp; Observability</p> </li> <li>Prometheus metrics integration</li> <li>Custom metrics for requests, latency, and tokens</li> <li>Grafana dashboard templates</li> <li> <p>Health check endpoints</p> </li> <li> <p>Database Integration</p> </li> <li>SQLModel with SQLite backend</li> <li>API key storage and management</li> <li>Automatic database initialization</li> <li>Migration support</li> </ul>"},{"location":"changelog/#infrastructure","title":"Infrastructure","text":"<ul> <li>Docker Support</li> <li>Multi-service Docker Compose setup</li> <li>Production-ready containers</li> <li>Volume persistence for data</li> <li> <p>Service health checks</p> </li> <li> <p>Development Tools</p> </li> <li>Comprehensive test suite (pytest)</li> <li>Code quality tools (black, isort, flake8, mypy)</li> <li>Pre-commit hooks</li> <li>Development environment setup</li> </ul>"},{"location":"changelog/#documentation","title":"Documentation","text":"<ul> <li>API Documentation</li> <li>OpenAPI/Swagger integration</li> <li>Interactive API explorer</li> <li>Request/response examples</li> <li> <p>Error code documentation</p> </li> <li> <p>User Guides</p> </li> <li>Installation instructions</li> <li>Configuration reference</li> <li>API usage examples</li> <li>Troubleshooting guide</li> </ul>"},{"location":"changelog/#030-2024-01-05","title":"[0.3.0] - 2024-01-05","text":""},{"location":"changelog/#added_2","title":"Added","text":"<ul> <li>Semantic caching with ChromaDB</li> <li>Vector similarity search</li> <li>Sentence transformer embeddings</li> <li>Cache analytics and monitoring</li> </ul>"},{"location":"changelog/#changed_1","title":"Changed","text":"<ul> <li>Improved cache key generation</li> <li>Enhanced error handling for vector operations</li> <li>Updated configuration schema</li> </ul>"},{"location":"changelog/#020-2024-01-04","title":"[0.2.0] - 2024-01-04","text":""},{"location":"changelog/#added_3","title":"Added","text":"<ul> <li>Google Gemini provider integration</li> <li>Anthropic Claude provider integration</li> <li>Provider translation layers</li> <li>Configuration-driven routing</li> </ul>"},{"location":"changelog/#changed_2","title":"Changed","text":"<ul> <li>Refactored provider architecture</li> <li>Improved request/response handling</li> <li>Enhanced model compatibility</li> </ul>"},{"location":"changelog/#010-2024-01-03","title":"[0.1.0] - 2024-01-03","text":""},{"location":"changelog/#added_4","title":"Added","text":"<ul> <li>Initial release</li> <li>OpenAI provider integration</li> <li>Basic FastAPI application</li> <li>Redis caching support</li> <li>Prometheus metrics</li> <li>API key authentication</li> <li>Rate limiting</li> <li>DLP scanning</li> </ul>"},{"location":"changelog/#infrastructure_1","title":"Infrastructure","text":"<ul> <li>Docker containerization</li> <li>Basic CI/CD pipeline</li> <li>SQLite database integration</li> </ul>"},{"location":"changelog/#version-history","title":"Version History","text":"Version Release Date Key Features 1.0.0 2024-01-06 Multi-provider, semantic caching, comprehensive testing 0.3.0 2024-01-05 Semantic caching, vector search 0.2.0 2024-01-04 Google &amp; Anthropic providers 0.1.0 2024-01-03 Initial release, OpenAI integration"},{"location":"changelog/#contributing","title":"Contributing","text":"<p>See our Testing Guide for development and testing information.</p>"},{"location":"changelog/#support","title":"Support","text":"<ul> <li>\ud83d\udcd6 Documentation</li> <li>\ud83d\udc1b Report Issues</li> <li>\ud83d\udcac Community Discussions</li> </ul>"},{"location":"troubleshooting/","title":"Troubleshooting Guide","text":"<p>Common issues and solutions for Prometheus Gateway.</p>"},{"location":"troubleshooting/#installation-issues","title":"\ud83d\udd27 Installation Issues","text":""},{"location":"troubleshooting/#python-version-errors","title":"Python Version Errors","text":"<p>Problem: <code>python: command not found</code> or version conflicts <pre><code># Solution: Use specific Python version\npython3.11 -m venv venv\n# or\npyenv install 3.11.13 &amp;&amp; pyenv local 3.11.13\n</code></pre></p>"},{"location":"troubleshooting/#dependency-installation-failures","title":"Dependency Installation Failures","text":"<p>Problem: <code>pip install</code> fails with compilation errors <pre><code># Solution: Update pip and use binary wheels\npython -m pip install --upgrade pip setuptools wheel\npip install --only-binary=all -r requirements.txt\n</code></pre></p>"},{"location":"troubleshooting/#spacy-model-download-issues","title":"spaCy Model Download Issues","text":"<p>Problem: <code>python -m spacy download en_core_web_lg</code> fails <pre><code># Solution: Manual download\npip install https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.7.1/en_core_web_lg-3.7.1-py3-none-any.whl\n</code></pre></p>"},{"location":"troubleshooting/#runtime-issues","title":"\ud83d\ude80 Runtime Issues","text":""},{"location":"troubleshooting/#server-wont-start","title":"Server Won't Start","text":"<p>Problem: <code>uvicorn</code> fails to start <pre><code># Check port availability\nlsof -i :8000\n\n# Kill existing processes\npkill -f uvicorn\n\n# Start with different port\nuvicorn app.main:app --port 8001\n</code></pre></p>"},{"location":"troubleshooting/#api-key-errors","title":"API Key Errors","text":"<p>Problem: <code>Environment variable OPENAI_API_KEY not set</code> <pre><code># Solution: Set environment variables\nexport OPENAI_API_KEY=your-key-here\n# or create .env file\necho \"OPENAI_API_KEY=your-key-here\" &gt; .env\n</code></pre></p>"},{"location":"troubleshooting/#database-issues","title":"Database Issues","text":"<p>Problem: SQLite database errors <pre><code># Solution: Reset database\nrm gateway.db\npython -c \"from app.database import create_db_and_tables; create_db_and_tables()\"\n</code></pre></p>"},{"location":"troubleshooting/#docker-issues","title":"\ud83d\udc33 Docker Issues","text":""},{"location":"troubleshooting/#container-build-failures","title":"Container Build Failures","text":"<p>Problem: Docker build fails <pre><code># Solution: Clean build\ndocker system prune -f\ndocker-compose build --no-cache\n</code></pre></p>"},{"location":"troubleshooting/#redis-connection-issues","title":"Redis Connection Issues","text":"<p>Problem: Redis connection refused <pre><code># Check Redis status\ndocker-compose logs redis\n\n# Restart Redis\ndocker-compose restart redis\n</code></pre></p>"},{"location":"troubleshooting/#permission-errors","title":"Permission Errors","text":"<p>Problem: File permission denied in containers <pre><code># Solution: Fix ownership\nsudo chown -R $USER:$USER .\n# or use Docker with user mapping\ndocker-compose run --user \"$(id -u):$(id -g)\" gateway\n</code></pre></p>"},{"location":"troubleshooting/#performance-issues","title":"\ud83d\udcca Performance Issues","text":""},{"location":"troubleshooting/#high-memory-usage","title":"High Memory Usage","text":"<p>Problem: Application uses too much RAM <pre><code># Monitor memory\ndocker stats\n\n# Solution: Adjust container limits\n# In docker-compose.yml:\nservices:\n  gateway:\n    mem_limit: 1G\n    memswap_limit: 1G\n</code></pre></p>"},{"location":"troubleshooting/#slow-response-times","title":"Slow Response Times","text":"<p>Problem: API responses are slow <pre><code># Check metrics\ncurl http://localhost:8000/metrics | grep duration\n\n# Solutions:\n# 1. Enable Redis caching\n# 2. Optimize database queries\n# 3. Scale horizontally\n</code></pre></p>"},{"location":"troubleshooting/#testing-issues","title":"\ud83d\udd0d Testing Issues","text":""},{"location":"troubleshooting/#tests-failing","title":"Tests Failing","text":"<p>Problem: <code>pytest</code> tests fail <pre><code># Solution: Check environment\nsource venv/bin/activate\npip install -r requirements-dev.txt\n\n# Run specific test\npytest tests/unit/test_dlp_functionality.py -v\n\n# Skip integration tests\npytest tests/unit/ -v\n</code></pre></p>"},{"location":"troubleshooting/#import-errors-in-tests","title":"Import Errors in Tests","text":"<p>Problem: <code>ModuleNotFoundError</code> in tests <pre><code># Solution: Install in development mode\npip install -e .\n\n# or adjust PYTHONPATH\nexport PYTHONPATH=\"${PYTHONPATH}:$(pwd)\"\n</code></pre></p>"},{"location":"troubleshooting/#network-issues","title":"\ud83c\udf10 Network Issues","text":""},{"location":"troubleshooting/#external-api-timeouts","title":"External API Timeouts","text":"<p>Problem: Provider APIs timing out <pre><code># Check connectivity\ncurl -I https://api.openai.com/v1/models\n\n# Solution: Configure timeouts in app/providers/\n</code></pre></p>"},{"location":"troubleshooting/#firewallproxy-issues","title":"Firewall/Proxy Issues","text":"<p>Problem: Blocked outbound connections <pre><code># Solution: Configure proxy\nexport HTTP_PROXY=http://proxy:8080\nexport HTTPS_PROXY=http://proxy:8080\n</code></pre></p>"},{"location":"troubleshooting/#logging-and-debugging","title":"\ud83d\udcdd Logging and Debugging","text":""},{"location":"troubleshooting/#enable-debug-logging","title":"Enable Debug Logging","text":"<pre><code># In app/main.py\nimport logging\nlogging.basicConfig(level=logging.DEBUG)\n</code></pre>"},{"location":"troubleshooting/#check-application-logs","title":"Check Application Logs","text":"<pre><code># Docker logs\ndocker-compose logs -f gateway\n\n# Local logs\ntail -f server.log\n</code></pre>"},{"location":"troubleshooting/#monitor-with-prometheus","title":"Monitor with Prometheus","text":"<pre><code># Check metrics\ncurl http://localhost:9090/api/v1/query?query=gateway_requests_total\n</code></pre>"},{"location":"troubleshooting/#getting-help","title":"\ud83c\udd98 Getting Help","text":""},{"location":"troubleshooting/#gather-debug-information","title":"Gather Debug Information","text":"<pre><code># System info\npython --version\ndocker --version\npip list | grep -E \"(fastapi|uvicorn|redis)\"\n\n# Check configuration\ncat config.yaml\nenv | grep -E \"(OPENAI|GOOGLE|ANTHROPIC)_API_KEY\"\n</code></pre>"},{"location":"troubleshooting/#report-issues","title":"Report Issues","text":"<p>When reporting issues, include:</p> <ol> <li>Environment: OS, Python version, Docker version</li> <li>Error message: Full traceback</li> <li>Steps to reproduce: Exact commands used</li> <li>Configuration: Sanitized config files</li> <li>Logs: Relevant log excerpts</li> </ol>"},{"location":"troubleshooting/#community-resources","title":"Community Resources","text":"<ul> <li>\ud83d\udcd6 Documentation</li> <li>\ud83d\udc1b Issue Tracker</li> <li>\ud83d\udcac Discussions</li> </ul>"},{"location":"troubleshooting/#common-solutions-checklist","title":"\ud83d\udccb Common Solutions Checklist","text":"<ul> <li> Virtual environment activated</li> <li> All dependencies installed</li> <li> Environment variables set</li> <li> API keys valid and active</li> <li> Ports not conflicting</li> <li> Database initialized</li> <li> Redis running (if using Docker)</li> <li> Firewall/proxy configured</li> <li> Sufficient disk space and memory</li> </ul>"},{"location":"api/endpoints/","title":"API Endpoints","text":"<p>Prometheus Gateway provides OpenAI-compatible endpoints plus additional management and monitoring endpoints.</p>"},{"location":"api/endpoints/#base-url","title":"Base URL","text":"<pre><code>http://localhost:8000\n</code></pre>"},{"location":"api/endpoints/#authentication","title":"Authentication","text":"<p>All API endpoints require authentication via the <code>X-API-Key</code> header:</p> <pre><code>curl -H \"X-API-Key: your-api-key\" http://localhost:8000/v1/chat/completions\n</code></pre>"},{"location":"api/endpoints/#chat-completions","title":"Chat Completions","text":""},{"location":"api/endpoints/#post-v1chatcompletions","title":"<code>POST /v1/chat/completions</code>","text":"<p>Main endpoint for chat completions, compatible with OpenAI API.</p> <p>Request Format: <pre><code>{\n  \"model\": \"gpt-3.5-turbo\",\n  \"messages\": [\n    {\"role\": \"user\", \"content\": \"Hello, world!\"}\n  ],\n  \"max_tokens\": 150,\n  \"temperature\": 0.7,\n  \"top_p\": 1.0,\n  \"n\": 1,\n  \"stream\": false,\n  \"stop\": null\n}\n</code></pre></p> <p>Response Format: <pre><code>{\n  \"id\": \"chatcmpl-1234567890\",\n  \"object\": \"chat.completion\",\n  \"created\": 1677652288,\n  \"model\": \"gpt-3.5-turbo\",\n  \"choices\": [\n    {\n      \"index\": 0,\n      \"message\": {\n        \"role\": \"assistant\",\n        \"content\": \"Hello! How can I help you today?\"\n      },\n      \"finish_reason\": \"stop\"\n    }\n  ],\n  \"usage\": {\n    \"prompt_tokens\": 12,\n    \"completion_tokens\": 8,\n    \"total_tokens\": 20\n  }\n}\n</code></pre></p> <p>Supported Models:</p> Provider Models OpenAI <code>gpt-4o</code>, <code>gpt-3.5-turbo</code>, <code>gpt-4-turbo</code>, <code>gpt-4</code> Google <code>gemini-2.5-flash</code>, <code>gemini-2.5-pro</code>, <code>gemini-1.5-pro</code> Anthropic <code>claude-sonnet-4-20250514</code>, <code>claude-opus-4-20250514</code> <p>Example with curl: <pre><code>curl -X POST http://localhost:8000/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -H \"X-API-Key: your-api-key\" \\\n  -d '{\n    \"model\": \"gpt-3.5-turbo\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n    \"max_tokens\": 50\n  }'\n</code></pre></p> <p>Example with Python: <pre><code>import requests\n\nresponse = requests.post(\n    \"http://localhost:8000/v1/chat/completions\",\n    headers={\n        \"Content-Type\": \"application/json\",\n        \"X-API-Key\": \"your-api-key\"\n    },\n    json={\n        \"model\": \"gpt-3.5-turbo\",\n        \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}],\n        \"max_tokens\": 50\n    }\n)\n\nprint(response.json())\n</code></pre></p>"},{"location":"api/endpoints/#health-and-status","title":"Health and Status","text":""},{"location":"api/endpoints/#get-health","title":"<code>GET /health</code>","text":"<p>Check service health status.</p> <p>Response: <pre><code>{\n  \"status\": \"healthy\",\n  \"timestamp\": \"2024-01-01T00:00:00Z\",\n  \"services\": {\n    \"redis\": \"connected\",\n    \"database\": \"connected\",\n    \"providers\": {\n      \"openai\": \"available\",\n      \"google\": \"available\",\n      \"anthropic\": \"available\"\n    }\n  },\n  \"version\": \"1.0.0\"\n}\n</code></pre></p>"},{"location":"api/endpoints/#get-healthready","title":"<code>GET /health/ready</code>","text":"<p>Kubernetes-style readiness probe.</p> <p>Response: <pre><code>{\n  \"ready\": true,\n  \"checks\": {\n    \"redis\": true,\n    \"database\": true,\n    \"config\": true\n  }\n}\n</code></pre></p>"},{"location":"api/endpoints/#get-healthlive","title":"<code>GET /health/live</code>","text":"<p>Kubernetes-style liveness probe.</p> <p>Response: <pre><code>{\n  \"alive\": true,\n  \"uptime\": 86400\n}\n</code></pre></p>"},{"location":"api/endpoints/#metrics-and-monitoring","title":"Metrics and Monitoring","text":""},{"location":"api/endpoints/#get-metrics","title":"<code>GET /metrics</code>","text":"<p>Prometheus metrics endpoint.</p> <p>Response: <pre><code># HELP gateway_requests_total Total number of requests\n# TYPE gateway_requests_total counter\ngateway_requests_total{method=\"POST\",endpoint=\"/v1/chat/completions\",status=\"200\"} 42\n\n# HELP gateway_request_duration_seconds Request duration in seconds\n# TYPE gateway_request_duration_seconds histogram\ngateway_request_duration_seconds_bucket{le=\"0.1\"} 10\ngateway_request_duration_seconds_bucket{le=\"0.5\"} 25\ngateway_request_duration_seconds_bucket{le=\"1.0\"} 35\ngateway_request_duration_seconds_bucket{le=\"+Inf\"} 42\n\n# HELP gateway_cache_hits_total Total number of cache hits\n# TYPE gateway_cache_hits_total counter\ngateway_cache_hits_total{cache_type=\"exact\"} 120\ngateway_cache_hits_total{cache_type=\"semantic\"} 45\n\n# HELP gateway_tokens_used_total Total number of tokens used\n# TYPE gateway_tokens_used_total counter\ngateway_tokens_used_total{provider=\"openai\",model=\"gpt-3.5-turbo\",type=\"prompt\"} 5000\ngateway_tokens_used_total{provider=\"openai\",model=\"gpt-3.5-turbo\",type=\"completion\"} 3000\n</code></pre></p>"},{"location":"api/endpoints/#get-stats","title":"<code>GET /stats</code>","text":"<p>Gateway statistics and analytics.</p> <p>Response: <pre><code>{\n  \"total_requests\": 1000,\n  \"cache_stats\": {\n    \"exact_cache\": {\n      \"hits\": 120,\n      \"misses\": 30,\n      \"hit_rate\": 0.8\n    },\n    \"semantic_cache\": {\n      \"hits\": 45,\n      \"misses\": 105,\n      \"hit_rate\": 0.3\n    }\n  },\n  \"provider_stats\": {\n    \"openai\": {\n      \"requests\": 500,\n      \"tokens\": 75000,\n      \"avg_latency\": 0.8\n    },\n    \"google\": {\n      \"requests\": 300,\n      \"tokens\": 45000,\n      \"avg_latency\": 0.6\n    },\n    \"anthropic\": {\n      \"requests\": 200,\n      \"tokens\": 30000,\n      \"avg_latency\": 1.2\n    }\n  },\n  \"rate_limit_stats\": {\n    \"requests_throttled\": 25,\n    \"top_users\": [\n      {\"api_key\": \"masked_key_1\", \"requests\": 150},\n      {\"api_key\": \"masked_key_2\", \"requests\": 120}\n    ]\n  }\n}\n</code></pre></p>"},{"location":"api/endpoints/#management","title":"Management","text":""},{"location":"api/endpoints/#post-adminreload-config","title":"<code>POST /admin/reload-config</code>","text":"<p>Reload configuration without restarting the service.</p> <p>Headers: - <code>X-API-Key</code>: Admin API key</p> <p>Response: <pre><code>{\n  \"status\": \"success\",\n  \"message\": \"Configuration reloaded successfully\",\n  \"timestamp\": \"2024-01-01T00:00:00Z\"\n}\n</code></pre></p>"},{"location":"api/endpoints/#get-admincachestats","title":"<code>GET /admin/cache/stats</code>","text":"<p>Detailed cache statistics.</p> <p>Response: <pre><code>{\n  \"redis\": {\n    \"connected\": true,\n    \"memory_usage\": \"150MB\",\n    \"keys\": 1250,\n    \"hits\": 850,\n    \"misses\": 400,\n    \"evictions\": 5\n  },\n  \"semantic_cache\": {\n    \"collection_size\": 500,\n    \"index_size\": \"25MB\",\n    \"queries\": 150,\n    \"hits\": 45,\n    \"avg_similarity\": 0.87\n  }\n}\n</code></pre></p>"},{"location":"api/endpoints/#post-admincacheclear","title":"<code>POST /admin/cache/clear</code>","text":"<p>Clear all caches.</p> <p>Request: <pre><code>{\n  \"cache_type\": \"all\"  // Options: \"all\", \"exact\", \"semantic\"\n}\n</code></pre></p> <p>Response: <pre><code>{\n  \"status\": \"success\",\n  \"message\": \"Cache cleared successfully\",\n  \"cleared\": {\n    \"exact_cache\": 1250,\n    \"semantic_cache\": 500\n  }\n}\n</code></pre></p>"},{"location":"api/endpoints/#error-responses","title":"Error Responses","text":"<p>All endpoints return structured error responses:</p> <pre><code>{\n  \"error\": {\n    \"code\": \"invalid_api_key\",\n    \"message\": \"Invalid API key provided\",\n    \"type\": \"authentication_error\"\n  },\n  \"timestamp\": \"2024-01-01T00:00:00Z\",\n  \"request_id\": \"req_1234567890\"\n}\n</code></pre> <p>Common Error Codes:</p> Code HTTP Status Description <code>invalid_api_key</code> 401 Invalid or missing API key <code>rate_limit_exceeded</code> 429 Rate limit exceeded <code>model_not_found</code> 400 Unsupported model <code>invalid_request</code> 400 Malformed request <code>provider_error</code> 502 Upstream provider error <code>internal_error</code> 500 Internal server error"},{"location":"api/endpoints/#rate-limiting","title":"Rate Limiting","text":"<p>Rate limiting is enforced per API key:</p> <ul> <li>Default: 10 requests per minute</li> <li>Headers: Rate limit info in response headers</li> <li>Retry: Use exponential backoff when rate limited</li> </ul> <p>Rate Limit Headers: <pre><code>X-RateLimit-Limit: 10\nX-RateLimit-Remaining: 7\nX-RateLimit-Reset: 1677652348\n</code></pre></p>"},{"location":"api/endpoints/#openapi-documentation","title":"OpenAPI Documentation","text":"<p>Interactive API documentation is available at: - Swagger UI: http://localhost:8000/docs - ReDoc: http://localhost:8000/redoc - OpenAPI JSON: http://localhost:8000/openapi.json</p>"},{"location":"api/endpoints/#client-libraries","title":"Client Libraries","text":""},{"location":"api/endpoints/#python","title":"Python","text":"<pre><code># Using OpenAI client (compatible)\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"your-api-key\",\n    base_url=\"http://localhost:8000/v1\"\n)\n\nresponse = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n</code></pre>"},{"location":"api/endpoints/#nodejs","title":"Node.js","text":"<pre><code>// Using OpenAI SDK\nconst OpenAI = require('openai');\n\nconst openai = new OpenAI({\n  apiKey: 'your-api-key',\n  baseURL: 'http://localhost:8000/v1'\n});\n\nconst response = await openai.chat.completions.create({\n  model: 'gpt-3.5-turbo',\n  messages: [{ role: 'user', content: 'Hello!' }]\n});\n</code></pre>"},{"location":"api/endpoints/#curl","title":"cURL","text":"<pre><code># Basic request\ncurl -X POST http://localhost:8000/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -H \"X-API-Key: your-api-key\" \\\n  -d '{\n    \"model\": \"gpt-3.5-turbo\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]\n  }'\n\n# With streaming\ncurl -X POST http://localhost:8000/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -H \"X-API-Key: your-api-key\" \\\n  -d '{\n    \"model\": \"gpt-3.5-turbo\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}],\n    \"stream\": true\n  }'\n</code></pre>"},{"location":"api/endpoints/#next-steps","title":"Next Steps","text":"<ul> <li>Configuration Guide - Configure your gateway</li> <li>Testing Guide - Test your implementation</li> <li>Troubleshooting - Common issues and solutions</li> </ul>"},{"location":"configuration/overview/","title":"Configuration Overview","text":"<p>Prometheus Gateway uses a flexible configuration system that supports both YAML files and environment variables.</p>"},{"location":"configuration/overview/#configuration-files","title":"Configuration Files","text":""},{"location":"configuration/overview/#main-configuration-configyaml","title":"Main Configuration: <code>config.yaml</code>","text":"<p>The primary configuration file defines providers, models, and routing rules:</p> <pre><code>providers:\n  - name: \"openai\"\n    api_key_env: \"OPENAI_API_KEY\"\n    models:\n      - \"gpt-4o\"\n      - \"gpt-3.5-turbo\"\n      - \"gpt-4-turbo\"\n      - \"gpt-4\"\n\n  - name: \"google\"\n    api_key_env: \"GOOGLE_API_KEY\"\n    models:\n      - \"gemini-2.5-flash\"\n      - \"gemini-2.5-pro\"\n      - \"gemini-1.5-pro\"\n\n  - name: \"anthropic\"\n    api_key_env: \"ANTHROPIC_API_KEY\"\n    models:\n      - \"claude-sonnet-4-20250514\"\n      - \"claude-opus-4-20250514\"\n      - \"claude-haiku-4-20250514\"\n</code></pre>"},{"location":"configuration/overview/#docker-compose-docker-composeyml","title":"Docker Compose: <code>docker-compose.yml</code>","text":"<p>Service orchestration and dependencies:</p> <pre><code>version: '3.8'\nservices:\n  gateway:\n    build: .\n    ports:\n      - \"8000:8000\"\n    depends_on:\n      - redis\n    environment:\n      - REDIS_HOST=redis\n      - OPENAI_API_KEY=${OPENAI_API_KEY}\n      - GOOGLE_API_KEY=${GOOGLE_API_KEY}\n      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}\n\n  redis:\n    image: redis:7-alpine\n    ports:\n      - \"6379:6379\"\n    volumes:\n      - redis_data:/data\n\n  prometheus:\n    image: prom/prometheus:latest\n    ports:\n      - \"9090:9090\"\n    volumes:\n      - ./prometheus.yml:/etc/prometheus/prometheus.yml\n    depends_on:\n      - gateway\n\n  grafana:\n    image: grafana/grafana:latest\n    ports:\n      - \"3000:3000\"\n    environment:\n      - GF_SECURITY_ADMIN_PASSWORD=admin\n    depends_on:\n      - prometheus\n\nvolumes:\n  redis_data:\n</code></pre>"},{"location":"configuration/overview/#environment-variables","title":"Environment Variables","text":""},{"location":"configuration/overview/#required-variables","title":"Required Variables","text":"<pre><code># LLM Provider API Keys\nOPENAI_API_KEY=sk-...                    # OpenAI API key\nGOOGLE_API_KEY=AI...                     # Google AI Studio API key\nANTHROPIC_API_KEY=sk-ant-...             # Anthropic API key\n</code></pre>"},{"location":"configuration/overview/#optional-variables","title":"Optional Variables","text":"<pre><code># Redis Configuration\nREDIS_HOST=localhost                     # Redis hostname\nREDIS_PORT=6379                         # Redis port\nREDIS_PASSWORD=your-password            # Redis password (optional)\nREDIS_DB=0                              # Redis database number\n\n# Caching Configuration\nCACHE_TTL=3600                          # Cache TTL in seconds\nSEMANTIC_CACHE_THRESHOLD=0.95           # Semantic similarity threshold\nSEMANTIC_CACHE_ENABLED=true             # Enable semantic caching\n\n# Rate Limiting\nRATE_LIMIT_REQUESTS=10                  # Requests per minute per API key\nRATE_LIMIT_WINDOW=60                    # Rate limit window in seconds\n\n# DLP Configuration\nDLP_ENABLED=true                        # Enable DLP scanning\nDLP_CONFIDENCE_THRESHOLD=0.8            # PII detection confidence threshold\n\n# Monitoring\nPROMETHEUS_METRICS_ENABLED=true         # Enable Prometheus metrics\nMETRICS_PORT=8000                       # Metrics endpoint port\n\n# Logging\nLOG_LEVEL=INFO                          # Logging level\nLOG_FORMAT=json                         # Log format (json|text)\n\n# Development\nDEBUG=false                             # Enable debug mode\nRELOAD=false                            # Auto-reload on code changes\n</code></pre>"},{"location":"configuration/overview/#configuration-validation","title":"Configuration Validation","text":"<p>The configuration is validated using Pydantic models:</p> <pre><code>from pydantic import BaseModel, Field\nfrom typing import List\n\nclass ProviderConfig(BaseModel):\n    name: str = Field(..., description=\"Provider name\")\n    api_key_env: str = Field(..., description=\"Environment variable name for API key\")\n    models: List[str] = Field(..., description=\"List of supported models\")\n\nclass Config(BaseModel):\n    providers: List[ProviderConfig] = Field(..., description=\"List of provider configurations\")\n</code></pre>"},{"location":"configuration/overview/#runtime-configuration","title":"Runtime Configuration","text":""},{"location":"configuration/overview/#adding-new-providers","title":"Adding New Providers","text":"<p>To add a new provider:</p> <ol> <li> <p>Update <code>config.yaml</code>: <pre><code>providers:\n  - name: \"new_provider\"\n    api_key_env: \"NEW_PROVIDER_API_KEY\"\n    models:\n      - \"new-model-1\"\n      - \"new-model-2\"\n</code></pre></p> </li> <li> <p>Create provider implementation: <pre><code># app/providers/new_provider/provider.py\nfrom app.providers.base import LLMProvider\n\nclass NewProvider(LLMProvider):\n    def __init__(self, api_key: str):\n        self.api_key = api_key\n\n    async def create_completion(self, request: ChatCompletionRequest) -&gt; dict:\n        # Implementation here\n        pass\n</code></pre></p> </li> <li> <p>Update factory in <code>app/services.py</code>: <pre><code>def get_provider(model: str) -&gt; LLMProvider:\n    # Add new provider mapping\n    if provider_config.name == \"new_provider\":\n        return NewProvider(api_key=api_key)\n</code></pre></p> </li> </ol>"},{"location":"configuration/overview/#dynamic-configuration-reload","title":"Dynamic Configuration Reload","text":"<p>The gateway supports configuration hot-reloading:</p> <pre><code># Send SIGHUP to reload configuration\nkill -HUP $(pgrep -f \"python.*main.py\")\n\n# Or use the API endpoint\ncurl -X POST http://localhost:8000/admin/reload-config \\\n  -H \"X-API-Key: your-admin-key\"\n</code></pre>"},{"location":"configuration/overview/#configuration-best-practices","title":"Configuration Best Practices","text":""},{"location":"configuration/overview/#security","title":"Security","text":"<ol> <li>Never commit API keys to version control</li> <li>Use environment variables for sensitive data</li> <li>Rotate API keys regularly</li> <li>Use different keys for different environments</li> </ol>"},{"location":"configuration/overview/#performance","title":"Performance","text":"<ol> <li>Tune cache TTL based on your use case</li> <li>Monitor cache hit rates and adjust thresholds</li> <li>Configure appropriate rate limits</li> <li>Use connection pooling for database connections</li> </ol>"},{"location":"configuration/overview/#monitoring","title":"Monitoring","text":"<ol> <li>Enable all metrics in production</li> <li>Set up alerting for key metrics</li> <li>Monitor resource usage regularly</li> <li>Keep logs structured for easy analysis</li> </ol>"},{"location":"configuration/overview/#example-configurations","title":"Example Configurations","text":""},{"location":"configuration/overview/#development","title":"Development","text":"<pre><code># config.yaml\nproviders:\n  - name: \"openai\"\n    api_key_env: \"OPENAI_API_KEY\"\n    models: [\"gpt-3.5-turbo\"]\n\n# .env\nOPENAI_API_KEY=sk-test-key\nDEBUG=true\nLOG_LEVEL=DEBUG\nCACHE_TTL=300\n</code></pre>"},{"location":"configuration/overview/#production","title":"Production","text":"<pre><code># config.yaml\nproviders:\n  - name: \"openai\"\n    api_key_env: \"OPENAI_API_KEY\"\n    models: [\"gpt-4o\", \"gpt-3.5-turbo\"]\n  - name: \"google\"\n    api_key_env: \"GOOGLE_API_KEY\"\n    models: [\"gemini-2.5-flash\", \"gemini-2.5-pro\"]\n  - name: \"anthropic\"\n    api_key_env: \"ANTHROPIC_API_KEY\"\n    models: [\"claude-sonnet-4-20250514\"]\n\n# .env\nOPENAI_API_KEY=sk-prod-key\nGOOGLE_API_KEY=AI-prod-key\nANTHROPIC_API_KEY=sk-ant-prod-key\nDEBUG=false\nLOG_LEVEL=INFO\nCACHE_TTL=3600\nPROMETHEUS_METRICS_ENABLED=true\n</code></pre>"},{"location":"configuration/overview/#high-availability","title":"High-Availability","text":"<pre><code># docker-compose.yml\nversion: '3.8'\nservices:\n  gateway:\n    deploy:\n      replicas: 3\n      restart_policy:\n        condition: on-failure\n        delay: 5s\n        max_attempts: 3\n      resources:\n        limits:\n          memory: 1G\n        reservations:\n          memory: 512M\n</code></pre>"},{"location":"configuration/overview/#next-steps","title":"Next Steps","text":"<ul> <li>API Reference - API documentation</li> <li>Testing Guide - Testing information</li> <li>Troubleshooting - Common issues and solutions</li> </ul>"},{"location":"development/testing/","title":"Testing Guide","text":"<p>Comprehensive testing strategy for Prometheus Gateway including unit tests, integration tests, and end-to-end testing.</p>"},{"location":"development/testing/#overview","title":"Overview","text":"<p>Our testing strategy covers: - Unit Tests: Individual components and functions - Integration Tests: Multi-component interactions - End-to-End Tests: Complete user workflows - Performance Tests: Load and stress testing - Security Tests: Vulnerability scanning</p>"},{"location":"development/testing/#test-structure","title":"Test Structure","text":"<pre><code>tests/\n\u251c\u2500\u2500 unit/                          # Unit tests\n\u2502   \u251c\u2500\u2500 test_openai_provider.py     # OpenAI provider tests\n\u2502   \u251c\u2500\u2500 test_google_provider.py     # Google provider tests\n\u2502   \u251c\u2500\u2500 test_anthropic_provider.py  # Anthropic provider tests\n\u2502   \u251c\u2500\u2500 test_routing_engine.py      # Routing logic tests\n\u2502   \u2514\u2500\u2500 test_dlp_functionality.py   # DLP scanning tests\n\u251c\u2500\u2500 integration/                   # Integration tests\n\u2502   \u251c\u2500\u2500 test_caching_systems.py    # Cache system tests\n\u2502   \u251c\u2500\u2500 test_end_to_end.py         # Complete workflows\n\u2502   \u2514\u2500\u2500 test_provider_integration.py # Provider integration\n\u251c\u2500\u2500 performance/                   # Performance tests\n\u2502   \u251c\u2500\u2500 test_load.py               # Load testing\n\u2502   \u2514\u2500\u2500 test_stress.py             # Stress testing\n\u251c\u2500\u2500 security/                      # Security tests\n\u2502   \u251c\u2500\u2500 test_auth.py               # Authentication tests\n\u2502   \u2514\u2500\u2500 test_vulnerabilities.py    # Security scanning\n\u2514\u2500\u2500 conftest.py                    # Test configuration\n</code></pre>"},{"location":"development/testing/#running-tests","title":"Running Tests","text":""},{"location":"development/testing/#all-tests","title":"All Tests","text":"<pre><code># Run all tests\npytest\n\n# Run with coverage\npytest --cov=app --cov-report=html --cov-report=term\n\n# Run with verbose output\npytest -v\n\n# Run specific test file\npytest tests/unit/test_openai_provider.py\n</code></pre>"},{"location":"development/testing/#test-categories","title":"Test Categories","text":"<pre><code># Unit tests only\npytest tests/unit/\n\n# Integration tests only\npytest tests/integration/\n\n# Performance tests\npytest tests/performance/\n\n# Security tests\npytest tests/security/\n\n# Tests by marker\npytest -m \"unit\"\npytest -m \"integration\"\npytest -m \"slow\"\npytest -m \"provider\"\n</code></pre>"},{"location":"development/testing/#parallel-testing","title":"Parallel Testing","text":"<pre><code># Run tests in parallel\npytest -n auto\n\n# Run with specific number of workers\npytest -n 4\n</code></pre>"},{"location":"development/testing/#unit-tests","title":"Unit Tests","text":""},{"location":"development/testing/#provider-tests","title":"Provider Tests","text":"<p>Testing individual provider implementations:</p> <pre><code>@pytest.mark.unit\n@pytest.mark.provider\nclass TestOpenAIProvider:\n    \"\"\"Test OpenAI provider functionality.\"\"\"\n\n    def test_provider_initialization(self):\n        \"\"\"Test provider initialization with API key.\"\"\"\n        provider = OpenAIProvider(api_key=\"test-key\")\n        assert provider.api_key == \"test-key\"\n\n    @pytest.mark.asyncio\n    async def test_create_completion_success(self, sample_chat_request):\n        \"\"\"Test successful completion creation.\"\"\"\n        provider = OpenAIProvider(api_key=\"test-key\")\n\n        with patch.object(provider, 'client') as mock_client:\n            mock_response = create_mock_openai_response(\"Test response\")\n            mock_client.chat.completions.create.return_value = mock_response\n\n            result = await provider.create_completion(sample_chat_request)\n\n            assert_openai_compatible_response(result)\n            assert result[\"choices\"][0][\"message\"][\"content\"] == \"Test response\"\n</code></pre>"},{"location":"development/testing/#routing-engine-tests","title":"Routing Engine Tests","text":"<p>Testing intelligent routing logic:</p> <pre><code>@pytest.mark.unit\n@pytest.mark.routing\nclass TestRoutingEngine:\n    \"\"\"Test routing engine functionality.\"\"\"\n\n    def test_get_provider_openai_models(self, test_config):\n        \"\"\"Test routing for OpenAI models.\"\"\"\n        with patch('app.services.config', test_config):\n            for model in [\"gpt-4o\", \"gpt-3.5-turbo\"]:\n                provider = get_provider(model)\n                assert isinstance(provider, OpenAIProvider)\n\n    def test_get_provider_unsupported_model(self, test_config):\n        \"\"\"Test error handling for unsupported models.\"\"\"\n        with patch('app.services.config', test_config):\n            with pytest.raises(ValueError) as exc_info:\n                get_provider(\"unsupported-model\")\n            assert \"No provider found\" in str(exc_info.value)\n</code></pre>"},{"location":"development/testing/#dlp-tests","title":"DLP Tests","text":"<p>Testing data loss prevention:</p> <pre><code>@pytest.mark.unit\n@pytest.mark.dlp\nclass TestDLPScanner:\n    \"\"\"Test DLP scanning functionality.\"\"\"\n\n    def test_scan_and_anonymize_email_addresses(self):\n        \"\"\"Test email detection and anonymization.\"\"\"\n        text = \"Contact john.doe@example.com for info\"\n        result = scan_and_anonymize_text(text)\n\n        assert \"[EMAIL_ADDRESS]\" in result\n        assert \"john.doe@example.com\" not in result\n\n    def test_scan_and_anonymize_multiple_pii_types(self):\n        \"\"\"Test multiple PII type detection.\"\"\"\n        text = \"Email: john@example.com, Phone: (555) 123-4567, SSN: 123-45-6789\"\n        result = scan_and_anonymize_text(text)\n\n        assert \"[EMAIL_ADDRESS]\" in result\n        assert \"[PHONE_NUMBER]\" in result\n        assert \"[US_SSN]\" in result\n</code></pre>"},{"location":"development/testing/#integration-tests","title":"Integration Tests","text":""},{"location":"development/testing/#cache-system-tests","title":"Cache System Tests","text":"<p>Testing two-level caching:</p> <pre><code>@pytest.mark.integration\n@pytest.mark.asyncio\nclass TestTwoLevelCaching:\n    \"\"\"Test complete caching system.\"\"\"\n\n    async def test_exact_cache_hit_flow(self, sample_request, mock_response):\n        \"\"\"Test exact cache hit bypasses providers.\"\"\"\n        with patch('app.cache.get_from_cache', return_value=mock_response):\n            result = await process_chat_completion(sample_request)\n            assert result == mock_response\n\n    async def test_semantic_cache_hit_flow(self, sample_request):\n        \"\"\"Test semantic cache hit after exact cache miss.\"\"\"\n        with patch('app.cache.get_from_cache', side_effect=[None, mock_response]):\n            with patch('app.vector_cache.search_semantic_cache', return_value=\"cache_key\"):\n                result = await process_chat_completion(sample_request)\n                assert result == mock_response\n</code></pre>"},{"location":"development/testing/#end-to-end-tests","title":"End-to-End Tests","text":"<p>Testing complete workflows:</p> <pre><code>@pytest.mark.integration\n@pytest.mark.e2e\nclass TestEndToEnd:\n    \"\"\"End-to-end workflow tests.\"\"\"\n\n    @pytest.mark.asyncio\n    async def test_complete_chat_workflow(self, client, valid_api_key):\n        \"\"\"Test complete chat completion workflow.\"\"\"\n        response = await client.post(\n            \"/v1/chat/completions\",\n            json={\n                \"model\": \"gpt-3.5-turbo\",\n                \"messages\": [{\"role\": \"user\", \"content\": \"Hello\"}],\n                \"max_tokens\": 50\n            },\n            headers={\"X-API-Key\": valid_api_key}\n        )\n\n        assert response.status_code == 200\n        data = response.json()\n        assert \"choices\" in data\n        assert len(data[\"choices\"]) &gt; 0\n        assert data[\"choices\"][0][\"message\"][\"content\"]\n</code></pre>"},{"location":"development/testing/#performance-tests","title":"Performance Tests","text":""},{"location":"development/testing/#load-testing","title":"Load Testing","text":"<pre><code>@pytest.mark.performance\n@pytest.mark.slow\nclass TestLoadPerformance:\n    \"\"\"Load testing scenarios.\"\"\"\n\n    @pytest.mark.asyncio\n    async def test_concurrent_requests(self, client, valid_api_key):\n        \"\"\"Test handling of concurrent requests.\"\"\"\n        async def make_request():\n            return await client.post(\n                \"/v1/chat/completions\",\n                json={\n                    \"model\": \"gpt-3.5-turbo\",\n                    \"messages\": [{\"role\": \"user\", \"content\": \"Test\"}],\n                    \"max_tokens\": 10\n                },\n                headers={\"X-API-Key\": valid_api_key}\n            )\n\n        # Send 50 concurrent requests\n        tasks = [make_request() for _ in range(50)]\n        responses = await asyncio.gather(*tasks)\n\n        # All should succeed\n        assert all(r.status_code == 200 for r in responses)\n\n        # Check response times\n        response_times = [r.elapsed.total_seconds() for r in responses]\n        avg_time = sum(response_times) / len(response_times)\n        assert avg_time &lt; 5.0  # Average under 5 seconds\n</code></pre>"},{"location":"development/testing/#memory-testing","title":"Memory Testing","text":"<pre><code>@pytest.mark.performance\nclass TestMemoryUsage:\n    \"\"\"Memory usage testing.\"\"\"\n\n    def test_memory_leak_detection(self):\n        \"\"\"Test for memory leaks in long-running processes.\"\"\"\n        import psutil\n        import gc\n\n        process = psutil.Process()\n        initial_memory = process.memory_info().rss\n\n        # Simulate heavy usage\n        for i in range(1000):\n            # Simulate request processing\n            large_data = [{\"content\": \"test\" * 1000} for _ in range(100)]\n            del large_data\n\n            if i % 100 == 0:\n                gc.collect()\n\n        final_memory = process.memory_info().rss\n        memory_growth = final_memory - initial_memory\n\n        # Memory growth should be reasonable\n        assert memory_growth &lt; 100 * 1024 * 1024  # Less than 100MB growth\n</code></pre>"},{"location":"development/testing/#security-tests","title":"Security Tests","text":""},{"location":"development/testing/#authentication-tests","title":"Authentication Tests","text":"<pre><code>@pytest.mark.security\nclass TestAuthentication:\n    \"\"\"Security testing for authentication.\"\"\"\n\n    @pytest.mark.asyncio\n    async def test_invalid_api_key(self, client):\n        \"\"\"Test rejection of invalid API keys.\"\"\"\n        response = await client.post(\n            \"/v1/chat/completions\",\n            json={\"model\": \"gpt-3.5-turbo\", \"messages\": []},\n            headers={\"X-API-Key\": \"invalid-key\"}\n        )\n\n        assert response.status_code == 401\n        assert \"invalid_api_key\" in response.json()[\"error\"][\"code\"]\n\n    @pytest.mark.asyncio\n    async def test_missing_api_key(self, client):\n        \"\"\"Test rejection of requests without API key.\"\"\"\n        response = await client.post(\n            \"/v1/chat/completions\",\n            json={\"model\": \"gpt-3.5-turbo\", \"messages\": []}\n        )\n\n        assert response.status_code == 401\n</code></pre>"},{"location":"development/testing/#vulnerability-tests","title":"Vulnerability Tests","text":"<pre><code>@pytest.mark.security\nclass TestVulnerabilities:\n    \"\"\"Security vulnerability testing.\"\"\"\n\n    @pytest.mark.asyncio\n    async def test_sql_injection_protection(self, client, valid_api_key):\n        \"\"\"Test SQL injection protection.\"\"\"\n        malicious_input = \"'; DROP TABLE users; --\"\n\n        response = await client.post(\n            \"/v1/chat/completions\",\n            json={\n                \"model\": \"gpt-3.5-turbo\",\n                \"messages\": [{\"role\": \"user\", \"content\": malicious_input}]\n            },\n            headers={\"X-API-Key\": valid_api_key}\n        )\n\n        # Should not crash or expose sensitive info\n        assert response.status_code in [200, 400]\n\n    @pytest.mark.asyncio\n    async def test_xss_protection(self, client, valid_api_key):\n        \"\"\"Test XSS protection.\"\"\"\n        xss_payload = \"&lt;script&gt;alert('xss')&lt;/script&gt;\"\n\n        response = await client.post(\n            \"/v1/chat/completions\",\n            json={\n                \"model\": \"gpt-3.5-turbo\",\n                \"messages\": [{\"role\": \"user\", \"content\": xss_payload}]\n            },\n            headers={\"X-API-Key\": valid_api_key}\n        )\n\n        # Should handle safely\n        assert response.status_code == 200\n        data = response.json()\n        # Response should not contain executable script\n        assert \"&lt;script&gt;\" not in str(data)\n</code></pre>"},{"location":"development/testing/#test-configuration","title":"Test Configuration","text":""},{"location":"development/testing/#fixtures","title":"Fixtures","text":"<pre><code># conftest.py\n@pytest.fixture\ndef sample_chat_request():\n    \"\"\"Sample chat completion request.\"\"\"\n    return ChatCompletionRequest(\n        model=\"gpt-3.5-turbo\",\n        messages=[{\"role\": \"user\", \"content\": \"What is the weather like today?\"}],\n        max_tokens=100,\n        temperature=0.7\n    )\n\n@pytest.fixture\ndef mock_redis():\n    \"\"\"Mock Redis client.\"\"\"\n    with patch('redis.Redis') as mock:\n        mock_client = Mock()\n        mock.return_value = mock_client\n        yield mock_client\n\n@pytest.fixture\ndef test_config():\n    \"\"\"Test configuration.\"\"\"\n    return Config(\n        providers=[\n            ProviderConfig(\n                name=\"openai\",\n                api_key_env=\"TEST_OPENAI_API_KEY\",\n                models=[\"gpt-4o\", \"gpt-3.5-turbo\"]\n            ),\n            ProviderConfig(\n                name=\"google\",\n                api_key_env=\"TEST_GOOGLE_API_KEY\",\n                models=[\"gemini-2.5-flash\", \"gemini-2.5-pro\"]\n            )\n        ]\n    )\n</code></pre>"},{"location":"development/testing/#environment-setup","title":"Environment Setup","text":"<pre><code># pytest.ini\n[tool:pytest]\ntestpaths = tests\naddopts = --cov=app --cov-report=html --cov-report=term-missing --cov-fail-under=80\npython_files = test_*.py\npython_classes = Test*\npython_functions = test_*\nmarkers =\n    unit: Unit tests\n    integration: Integration tests\n    performance: Performance tests\n    security: Security tests\n    slow: Slow tests\n    provider: Provider tests\n    dlp: DLP tests\n    routing: Routing tests\n    redis: Redis tests\n    vector: Vector cache tests\n    e2e: End-to-end tests\nasyncio_mode = auto\n</code></pre>"},{"location":"development/testing/#continuous-integration","title":"Continuous Integration","text":""},{"location":"development/testing/#github-actions","title":"GitHub Actions","text":"<pre><code># .github/workflows/test.yml\nname: Test Suite\non: [push, pull_request]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    strategy:\n      matrix:\n        python-version: [3.9, 3.10, 3.11]\n\n    steps:\n    - uses: actions/checkout@v4\n    - name: Set up Python ${{ matrix.python-version }}\n      uses: actions/setup-python@v4\n      with:\n        python-version: ${{ matrix.python-version }}\n\n    - name: Install dependencies\n      run: |\n        pip install -r requirements.txt\n        pip install -r requirements-dev.txt\n\n    - name: Run tests\n      run: pytest --cov=app --cov-report=xml\n\n    - name: Upload coverage\n      uses: codecov/codecov-action@v3\n</code></pre>"},{"location":"development/testing/#test-data-management","title":"Test Data Management","text":""},{"location":"development/testing/#fixtures-and-mocks","title":"Fixtures and Mocks","text":"<pre><code># Test data for consistent testing\nSAMPLE_RESPONSES = {\n    \"openai\": {\n        \"id\": \"chatcmpl-test123\",\n        \"object\": \"chat.completion\",\n        \"created\": 1677652288,\n        \"model\": \"gpt-3.5-turbo\",\n        \"choices\": [\n            {\n                \"index\": 0,\n                \"message\": {\n                    \"role\": \"assistant\",\n                    \"content\": \"Test response\"\n                },\n                \"finish_reason\": \"stop\"\n            }\n        ],\n        \"usage\": {\n            \"prompt_tokens\": 10,\n            \"completion_tokens\": 5,\n            \"total_tokens\": 15\n        }\n    }\n}\n\n# Mock responses for different providers\ndef create_mock_openai_response(content: str) -&gt; dict:\n    \"\"\"Create mock OpenAI response.\"\"\"\n    response = SAMPLE_RESPONSES[\"openai\"].copy()\n    response[\"choices\"][0][\"message\"][\"content\"] = content\n    return response\n</code></pre>"},{"location":"development/testing/#best-practices","title":"Best Practices","text":""},{"location":"development/testing/#test-organization","title":"Test Organization","text":"<ol> <li>Group related tests in classes</li> <li>Use descriptive test names that explain what's being tested</li> <li>Test one thing at a time - focused assertions</li> <li>Use fixtures for common test data</li> <li>Mock external dependencies to avoid flaky tests</li> </ol>"},{"location":"development/testing/#test-quality","title":"Test Quality","text":"<ol> <li>Test edge cases and error conditions</li> <li>Verify both positive and negative scenarios</li> <li>Test with realistic data that matches production</li> <li>Include performance assertions for critical paths</li> <li>Test security boundaries and input validation</li> </ol>"},{"location":"development/testing/#coverage-goals","title":"Coverage Goals","text":"<ul> <li>Unit Tests: 90%+ coverage</li> <li>Integration Tests: Critical paths covered</li> <li>E2E Tests: Main user workflows</li> <li>Performance Tests: Load and stress scenarios</li> <li>Security Tests: Authentication and input validation</li> </ul>"},{"location":"development/testing/#running-tests-locally","title":"Running Tests Locally","text":"<pre><code># Install test dependencies\npip install -r requirements-dev.txt\n\n# Run quick unit tests\npytest tests/unit/\n\n# Run all tests with coverage\npytest --cov=app --cov-report=html\n\n# Run specific test types\npytest -m \"unit and not slow\"\npytest -m \"integration\"\npytest -m \"security\"\n\n# Run with test output\npytest -v -s\n\n# Run failed tests only\npytest --lf\n\n# Run tests in parallel\npytest -n auto\n</code></pre>"},{"location":"development/testing/#debugging-tests","title":"Debugging Tests","text":"<pre><code># Run with debugger\npytest --pdb\n\n# Run with extra output\npytest -vvs\n\n# Run specific test with debugging\npytest -vvs tests/unit/test_openai_provider.py::TestOpenAIProvider::test_create_completion_success\n\n# Profile test performance\npytest --profile\n</code></pre>"},{"location":"development/testing/#next-steps","title":"Next Steps","text":"<ul> <li>Configuration Guide - Configuration reference</li> <li>API Reference - API documentation</li> <li>Troubleshooting - Common issues and solutions</li> </ul>"},{"location":"getting-started/installation/","title":"Installation Guide","text":"<p>This guide will help you get Prometheus Gateway up and running in different environments.</p>"},{"location":"getting-started/installation/#system-requirements","title":"System Requirements","text":"<ul> <li>Python: 3.9 or higher</li> <li>Redis: 6.0 or higher</li> <li>Memory: 2GB RAM minimum, 4GB recommended</li> <li>Storage: 1GB free space for dependencies and models</li> </ul>"},{"location":"getting-started/installation/#installation-methods","title":"Installation Methods","text":""},{"location":"getting-started/installation/#docker-recommended","title":"\ud83d\udc33 Docker (Recommended)","text":"<p>The easiest way to get started is with Docker Compose:</p> <pre><code># Clone the repository\ngit clone https://github.com/ozanunal0/prometheus-gateway.git\ncd prometheus-gateway\n\n# Start all services\ndocker-compose up -d\n\n# Check service status\ndocker-compose ps\n\n# View logs\ndocker-compose logs -f gateway\n</code></pre> <p>Services included: - <code>gateway</code>: Main FastAPI application - <code>redis</code>: Caching layer - <code>prometheus</code>: Metrics collection - <code>grafana</code>: Monitoring dashboard</p>"},{"location":"getting-started/installation/#local-development","title":"\ud83d\udc0d Local Development","text":"<p>For development or customization:</p> <pre><code># Clone the repository\ngit clone https://github.com/ozanunal0/prometheus-gateway.git\ncd prometheus-gateway\n\n# Create virtual environment\npython -m venv venv\nsource venv/bin/activate  # Linux/macOS\n# venv\\Scripts\\activate   # Windows\n\n# Install dependencies\npip install -r requirements.txt\n\n# Download required spaCy model\npython -m spacy download en_core_web_lg\n\n# Start Redis (required)\nredis-server\n\n# Run the application\nuvicorn app.main:app --host 0.0.0.0 --port 8000 --reload\n</code></pre>"},{"location":"getting-started/installation/#production-deployment","title":"\ud83d\udce6 Production Deployment","text":"<p>For production environments:</p> <pre><code># Install with production dependencies\npip install -r requirements.txt\n\n# Run with production ASGI server\ngunicorn app.main:app -w 4 -k uvicorn.workers.UvicornWorker --bind 0.0.0.0:8000\n</code></pre>"},{"location":"getting-started/installation/#environment-variables","title":"Environment Variables","text":"<p>Create a <code>.env</code> file or set these environment variables:</p> <pre><code># Required API Keys\nOPENAI_API_KEY=your-openai-api-key\nGOOGLE_API_KEY=your-google-api-key\nANTHROPIC_API_KEY=your-anthropic-api-key\n\n# Redis Configuration\nREDIS_HOST=localhost\nREDIS_PORT=6379\nREDIS_PASSWORD=your-redis-password  # Optional\n\n# Optional: Pre-defined API Keys for testing\nAPI_KEYS=key1,key2,key3\n</code></pre>"},{"location":"getting-started/installation/#api-key-setup","title":"API Key Setup","text":"<p>Generate your first API key:</p> <pre><code># Create an API key\npython create_key.py user@example.com\n\n# Example output:\n# API Key: gw_1234567890abcdef1234567890abcdef\n# Owner: user@example.com\n# Created: 2024-01-01T00:00:00Z\n</code></pre>"},{"location":"getting-started/installation/#verification","title":"Verification","text":"<p>Test your installation:</p> <pre><code># Health check\ncurl http://localhost:8000/health\n\n# Test chat completion\ncurl -X POST http://localhost:8000/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -H \"X-API-Key: your-api-key\" \\\n  -d '{\n    \"model\": \"gpt-3.5-turbo\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Hello, world!\"}],\n    \"max_tokens\": 50\n  }'\n</code></pre>"},{"location":"getting-started/installation/#access-services","title":"Access Services","text":"<p>Once running, you can access:</p> <ul> <li>Gateway API: http://localhost:8000</li> <li>API Documentation: http://localhost:8000/docs</li> <li>Prometheus: http://localhost:9090</li> <li>Grafana: http://localhost:3000 (admin/admin)</li> </ul>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/installation/#common-issues","title":"Common Issues","text":"<p>Redis Connection Error <pre><code># Check Redis is running\nredis-cli ping\n# Should return: PONG\n\n# Check Redis logs\nredis-cli monitor\n</code></pre></p> <p>spaCy Model Missing <pre><code># Download the required model\npython -m spacy download en_core_web_lg\n\n# Verify installation\npython -c \"import spacy; nlp = spacy.load('en_core_web_lg'); print('Model loaded successfully')\"\n</code></pre></p> <p>Port Already in Use <pre><code># Find process using port 8000\nlsof -i :8000\n\n# Kill the process\nkill -9 &lt;PID&gt;\n</code></pre></p> <p>Memory Issues <pre><code># Check memory usage\nfree -h\n\n# Increase swap space if needed\nsudo fallocate -l 2G /swapfile\nsudo mkswap /swapfile\nsudo swapon /swapfile\n</code></pre></p>"},{"location":"getting-started/installation/#docker-issues","title":"Docker Issues","text":"<p>Container Won't Start <pre><code># Check Docker logs\ndocker-compose logs gateway\n\n# Restart services\ndocker-compose restart\n\n# Rebuild containers\ndocker-compose build --no-cache\n</code></pre></p> <p>Permission Errors <pre><code># Fix file permissions\nsudo chown -R $USER:$USER .\n\n# On Linux, you might need to adjust SELinux\nsudo setsebool -P httpd_can_network_connect 1\n</code></pre></p>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<ol> <li>Quick Start Guide - Test your installation</li> <li>Configuration Guide - Customize your setup</li> <li>API Reference - Explore available endpoints</li> </ol>"},{"location":"getting-started/quickstart/","title":"Quick Start Guide","text":"<p>Get up and running with Prometheus Gateway in just a few minutes!</p>"},{"location":"getting-started/quickstart/#1-minute-setup","title":"\ud83d\ude80 1-Minute Setup","text":""},{"location":"getting-started/quickstart/#option-a-docker-compose-recommended","title":"Option A: Docker Compose (Recommended)","text":"<pre><code># Clone the repository\ngit clone https://github.com/ozanunal0/prometheus-gateway.git\ncd prometheus-gateway\n\n# Set your API keys\nexport OPENAI_API_KEY=your-openai-key\nexport GOOGLE_API_KEY=your-google-key\nexport ANTHROPIC_API_KEY=your-anthropic-key\n\n# Start all services\ndocker-compose up -d\n\n# Create your first API key\npython create_key.py your-email@example.com\n</code></pre>"},{"location":"getting-started/quickstart/#option-b-local-development","title":"Option B: Local Development","text":"<pre><code># Clone and setup\ngit clone https://github.com/ozanunal0/prometheus-gateway.git\ncd prometheus-gateway\n\n# Create virtual environment\npython -m venv venv\nsource venv/bin/activate  # Linux/macOS\n\n# Install dependencies\npip install -r requirements.txt\npython -m spacy download en_core_web_lg\n\n# Set environment variables\nexport OPENAI_API_KEY=your-openai-key\n\n# Run the server\nuvicorn app.main:app --reload\n</code></pre>"},{"location":"getting-started/quickstart/#first-api-call","title":"\ud83c\udfaf First API Call","text":"<p>Once running, test with your first API call:</p> <pre><code># Create API key (save the output!)\npython create_key.py test-user@example.com\n\n# Test API call\ncurl -X POST http://localhost:8000/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -H \"X-API-Key: YOUR_API_KEY_HERE\" \\\n  -d '{\n    \"model\": \"gpt-3.5-turbo\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Hello, world!\"}],\n    \"max_tokens\": 50\n  }'\n</code></pre>"},{"location":"getting-started/quickstart/#access-services","title":"\ud83d\udcca Access Services","text":"<ul> <li>API Gateway: http://localhost:8000</li> <li>API Documentation: http://localhost:8000/docs</li> <li>Prometheus Metrics: http://localhost:9090</li> <li>Grafana Dashboard: http://localhost:3000 (admin/admin)</li> </ul>"},{"location":"getting-started/quickstart/#verify-installation","title":"\u2705 Verify Installation","text":"<pre><code># Check health\ncurl http://localhost:8000/metrics\n\n# Run tests\npytest tests/unit/test_dlp_functionality.py -v\n</code></pre>"},{"location":"getting-started/quickstart/#configuration","title":"\ud83d\udd27 Configuration","text":"<p>Basic configuration in <code>config.yaml</code>:</p> <pre><code>providers:\n  - name: \"openai\"\n    api_key_env: \"OPENAI_API_KEY\"\n    models: [\"gpt-3.5-turbo\", \"gpt-4\"]\n</code></pre>"},{"location":"getting-started/quickstart/#next-steps","title":"\ud83d\udcda Next Steps","text":"<ul> <li>Configuration Guide</li> <li>API Reference</li> <li>Testing Guide</li> </ul>"}]}