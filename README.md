![banner.png](public/banner.png)
# üöÄ Prometheus GENAI API Gateway

> Enterprise-grade LLM API Gateway with built-in privacy protection, caching, and observability

[![Python 3.11](https://img.shields.io/badge/python-3.11-blue.svg)](https://python.org)
[![FastAPI](https://img.shields.io/badge/FastAPI-005571?logo=fastapi)](https://fastapi.tiangolo.com)
[![Docker](https://img.shields.io/badge/docker-%230db7ed.svg?logo=docker&logoColor=white)](https://docker.com)
[![Redis](https://img.shields.io/badge/redis-%23DD0031.svg?logo=redis&logoColor=white)](https://redis.io)
[![Prometheus](https://img.shields.io/badge/Prometheus-E6522C?logo=Prometheus&logoColor=white)](https://prometheus.io)

## ‚ú® Features

- **üîí Privacy First**: Automatic PII detection and anonymization using Microsoft Presidio
- **‚ö° Smart Caching**: Redis-powered response caching to reduce API costs
- **üìä Full Observability**: Comprehensive Prometheus metrics + Grafana dashboards
- **üõ°Ô∏è Enterprise Security**: API key authentication with rate limiting
- **üîå Multi-Provider Ready**: Extensible architecture for multiple LLM providers
- **üöÄ OpenAI Compatible**: Drop-in replacement for OpenAI API endpoints

## üèóÔ∏è Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Client    ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ     DLP      ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇRate Limiter ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ    Auth     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ  Middleware  ‚îÇ    ‚îÇ Middleware  ‚îÇ    ‚îÇ Middleware  ‚îÇ
                   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                                    ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Redis     ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÇ    Cache     ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÇ   Provider  ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÇ   Gateway   ‚îÇ
‚îÇ   Cache     ‚îÇ    ‚îÇ    Layer     ‚îÇ    ‚îÇ   Factory   ‚îÇ    ‚îÇ   Handler   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üöÄ Quick Start

### Docker Compose (Recommended)

```bash
# Clone the repository
git clone https://github.com/ozanunal0/prometheus-gateway.git
cd prometheus-gateway

# Set your OpenAI API key
echo "OPENAI_API_KEY=your_key_here" > .env

# Start all services
docker-compose up -d

# Create an API key
docker-compose exec gateway python create_key.py your@email.com
```

### Manual Setup

```bash
# Setup environment
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
python -m spacy download en_core_web_lg

# Run the gateway
uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
```

## üéØ Usage

```bash
# Make a request (OpenAI compatible)
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "X-API-Key: your_api_key_here" \
  -d '{
    "model": "gpt-3.5-turbo",
    "messages": [{"role": "user", "content": "Hello!"}],
    "max_tokens": 100
  }'
```

## üìä Monitoring

Access your monitoring stack:

- **Gateway**: http://localhost:8000
- **Prometheus**: http://localhost:9090
- **Grafana**: http://localhost:3000 (admin/admin)

## üîß Configuration

| Environment Variable | Description | Default |
|---------------------|-------------|---------|
| `OPENAI_API_KEY` | OpenAI API key | Required |
| `REDIS_HOST` | Redis hostname | localhost |
| `REDIS_PORT` | Redis port | 6379 |
| `REDIS_PASSWORD` | Redis password | None |

## üõ°Ô∏è Privacy & Security

- **PII Detection**: Automatically detects and anonymizes emails, phone numbers, SSNs, credit cards
- **Secure API Keys**: SHA-256 hashed storage, never logged in plaintext
- **Rate Limiting**: Configurable per-key rate limits (default: 10/minute)
- **Request Isolation**: Each request is processed independently

## üìà Metrics

Track your usage with built-in metrics:

- Request counts by owner, model, and status
- Response latency histograms
- Token usage tracking (prompt, completion, total)
- Error rates and performance insights

## üîå Extending

Add new LLM providers easily:

```python
from app.providers.base import LLMProvider

class YourProvider(LLMProvider):
    async def chat_completions(self, request):
        # Your implementation
        pass
```

## ü§ù Contributing

This project is in active development. Contributions are welcome!

---
## ‚ö†Ô∏è Security Disclaimer

This project is built with a "security-first" mindset, incorporating best practices like PII scrubbing, authentication, and rate limiting. However, it is an open-source tool provided "as-is" and **has not been subjected to a professional, third-party security audit.**

Security is complex. While Prometheus Gateway can be a powerful tool to enhance your security posture for LLM applications, it is **not a substitute for a comprehensive security assessment** of your own infrastructure and processes. Please perform your own security validation before using this project in production environments with highly sensitive data.

I welcome feedback, vulnerability reports, and contributions from the security community to make this project more robust.

---


<div align="center">
  <sub>Built with ‚ù§Ô∏è for the LLM community</sub>
</div>